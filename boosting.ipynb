{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzOPGQHRt4oOlaldAyHe5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssj108/Files-exceptional-handling-logging-and-memory-management-assignment/blob/main/boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1) What is Boosting in Machine Learning? How does it improve weak learners?\n",
        "\n",
        "Answer:\n",
        "Boosting is an ensemble learning technique that combines multiple weak learners (usually shallow decision trees) into a strong learner.\n",
        "\n",
        "Each new learner is trained to focus on the mistakes of the previous ones.\n",
        "\n",
        "By giving higher weights to misclassified samples, boosting ensures that difficult cases get more attention.\n",
        "\n",
        "Final predictions are made by taking a weighted vote (classification) or weighted sum (regression) of all learners.\n",
        "\n",
        "\n",
        "Improvement: Boosting converts low-bias, high-variance weak models into a strong model with lower error and better generalization.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Q2) Explain the difference between AdaBoost and Gradient Boosting in terms of training process.\n",
        "\n",
        "Answer:\n",
        "\n",
        "AdaBoost (Adaptive Boosting):\n",
        "\n",
        "Assigns weights to training samples.\n",
        "\n",
        "Misclassified samples get higher weights in the next iteration.\n",
        "\n",
        "Learners are combined using weighted voting.\n",
        "\n",
        "Optimizes exponential loss.\n",
        "\n",
        "\n",
        "Gradient Boosting:\n",
        "\n",
        "Builds models sequentially, each fitting the residual errors of the previous model.\n",
        "\n",
        "Uses gradient descent in function space to minimize a chosen loss (e.g., MSE, log loss).\n",
        "\n",
        "More flexible (can use different losses, learning rates, subsampling, etc.).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Q3) How does regularization help in XGBoost?\n",
        "\n",
        "Answer:\n",
        "XGBoost uses multiple regularization techniques to prevent overfitting:\n",
        "\n",
        "1. L1 (Lasso) and L2 (Ridge) penalties on leaf weights.\n",
        "\n",
        "\n",
        "2. Tree complexity control: parameter gamma requires a minimum loss reduction to allow a split.\n",
        "\n",
        "\n",
        "3. Shrinkage (learning_rate) slows learning, allowing more accurate additive models.\n",
        "\n",
        "\n",
        "4. Subsampling (row/column sampling): reduces variance and prevents overfitting.\n",
        "\n",
        "\n",
        "\n",
        "Together, these mechanisms ensure XGBoost models are robust, generalizable, and accurate.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Q4) Why is CatBoost efficient for handling categorical data?\n",
        "\n",
        "Answer:\n",
        "CatBoost is designed to natively handle categorical features without requiring one-hot encoding.\n",
        "\n",
        "It uses ordered target statistics (a type of target encoding) with random permutations to avoid target leakage.\n",
        "\n",
        "Handles high-cardinality categorical features efficiently.\n",
        "\n",
        "Supports missing values naturally.\n",
        "\n",
        "Reduces preprocessing effort and often improves performance on tabular datasets with mixed feature types.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Q5) Mention real-world applications where Boosting is preferred over Bagging.\n",
        "\n",
        "Answer:\n",
        "Boosting is often better when the dataset has complex patterns and subtle signals:\n",
        "\n",
        "Fraud detection in financial transactions.\n",
        "\n",
        "Credit risk prediction (loan defaults).\n",
        "\n",
        "Medical diagnosis (disease prediction).\n",
        "\n",
        "Click-through rate (CTR) prediction in online advertising.\n",
        "\n",
        "Customer churn prediction in telecom/retail.\n",
        "\n",
        "\n",
        "Boosting usually outperforms bagging (like Random Forests) when high accuracy and fine-grained decision boundaries are required.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Q6) Python Program – AdaBoost Classifier on Breast Cancer Dataset"
      ],
      "metadata": {
        "id": "x4wHSbQGP35_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1io8OXe-PzJM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train AdaBoost\n",
        "model = AdaBoostClassifier(n_estimators=200, learning_rate=0.5, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7) Python Program – Gradient Boosting Regressor on California Housing Dataset"
      ],
      "metadata": {
        "id": "cfuu2L_oQzyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting\n",
        "model = GradientBoostingRegressor(n_estimators=400, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# R² score\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Gradient Boosting R²:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "geHoT4m0Q6eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) Python Program – XGBoost Classifier on Breast Cancer Dataset with Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "n8lHCxVuRGBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Grid Search for learning rate\n",
        "param_grid = {'learning_rate': [0.05, 0.1, 0.2, 0.3]}\n",
        "grid = GridSearchCV(xgb, param_grid, scoring='accuracy', cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Test Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "i08h5gMERifo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9) Python Program – CatBoost Classifier on Breast Cancer Dataset (Confusion Matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "1S79fxN_Roef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train CatBoost\n",
        "model = CatBoostClassifier(iterations=400, depth=4, learning_rate=0.1, loss_function='Logloss', verbose=False, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uMkzJwRhRw4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10) Case Study – Loan Default Prediction using Boosting\n",
        "\n",
        "Answer:\n",
        "\n",
        "1. Preprocessing:\n",
        "\n",
        "Handle missing values (median imputation for numerical, mode/target encoding for categorical).\n",
        "\n",
        "Encode categorical variables (CatBoost can handle natively, otherwise one-hot/target encoding).\n",
        "\n",
        "Standardize numeric features if needed.\n",
        "\n",
        "\n",
        "\n",
        "2. Model Choice:\n",
        "\n",
        "Use CatBoost (handles categorical + missing values efficiently).\n",
        "\n",
        "Alternatively, XGBoost with proper encoding and class weighting.\n",
        "\n",
        "\n",
        "\n",
        "3. Handling Class Imbalance:\n",
        "\n",
        "Use class weights (scale_pos_weight in XGBoost, class_weights in CatBoost).\n",
        "\n",
        "Oversample minority class (SMOTE) if required.\n",
        "\n",
        "\n",
        "\n",
        "4. Evaluation Metrics:\n",
        "ROC-AUC → overall ranking ability.\n",
        "\n",
        "Precision, Recall, F1-score → balance between false positives and false negatives.\n",
        "\n",
        "PR-AUC → useful for highly imbalanced datasets.\n",
        "\n",
        "\n",
        "\n",
        "5. Business Impact:\n",
        "\n",
        "Reduces risk of approving loans likely to default.\n",
        "\n",
        "Improves profitability by targeting safe customers.\n",
        "\n",
        "Ensures compliance with financial regulations.\n",
        "\n",
        "Builds trust with stakeholders through accurate, explainable models."
      ],
      "metadata": {
        "id": "zIbwoeYxR_md"
      }
    }
  ]
}