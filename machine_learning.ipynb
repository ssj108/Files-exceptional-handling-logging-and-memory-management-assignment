{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOljmkGAteVk8JXSKlPUyL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssj108/Files-exceptional-handling-logging-and-memory-management-assignment/blob/main/machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Answer:\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. In classification, it splits the dataset into subsets based on feature values, forming a tree structure where:\n",
        "\n",
        "Each internal node represents a feature test.\n",
        "\n",
        "Each branch represents an outcome of that test.\n",
        "\n",
        "Each leaf node represents a class label.\n",
        "\n",
        "\n",
        "The model makes decisions by traversing from the root to a leaf, following the path determined by the feature values of the input data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini Impurity measures the likelihood of incorrectly classifying a randomly chosen element.\n",
        "Formula:\n",
        "\n",
        "Entropy measures the amount of information or uncertainty in the dataset.\n",
        "Formula:\n",
        "\n",
        "\n",
        "These measures help determine the quality of a split. The lower the impurity after the split, the better. The tree algorithm chooses the feature and threshold that leads to the greatest reduction in impurity.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning stops the tree from growing once a condition is met (e.g., max depth, min samples).\n",
        "Advantage: Prevents overfitting early and reduces training time.\n",
        "\n",
        "Post-Pruning allows the tree to fully grow and then removes branches that don't improve performance.\n",
        "Advantage: Leads to simpler models that generalize better.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:\n",
        "Information Gain is the reduction in impurity (Entropy or Gini) achieved after a dataset is split on a feature. It’s calculated as:\n",
        "\n",
        "\\text{Information Gain} = \\text{Entropy (parent)} - \\sum_{i} \\left( \\frac{n_i}{n} \\cdot \\text{Entropy (child}_i) \\right)\n",
        "\n",
        "It helps the Decision Tree select the feature that provides the most informative split, improving the model's classification ability.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer:\n",
        "Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Customer segmentation\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to understand and interpret\n",
        "\n",
        "Requires little data preprocessing\n",
        "\n",
        "Handles both numerical and categorical data\n",
        "\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Unstable with small data variations\n",
        "\n",
        "Can create biased trees with imbalanced data\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Question 6: Python Program (Iris Dataset, Gini Criterion)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n"
      ],
      "metadata": {
        "id": "zrD6V3GpViik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)\n"
      ],
      "metadata": {
        "id": "AK_fDBoCV67T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Python Program (Iris Dataset, max_depth=3 vs full tree)\n"
      ],
      "metadata": {
        "id": "Yhurut-_WCMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Tree\n",
        "full_model = DecisionTreeClassifier()\n",
        "full_model.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_model.predict(X_test))\n",
        "\n",
        "# Pruned Tree\n",
        "pruned_model = DecisionTreeClassifier(max_depth=3)\n",
        "pruned_model.fit(X_train, y_train)\n",
        "pruned_acc = accuracy_score(y_test, pruned_model.predict(X_test))\n",
        "\n",
        "print(\"Full Tree Accuracy:\", full_acc)\n",
        "print(\"Pruned Tree Accuracy:\", pruned_acc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWYMxpnsWKXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Python Program (Boston Housing, Regression)\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n"
      ],
      "metadata": {
        "id": "vm9uJF3aWP3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boston = load_boston()\n",
        "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n",
        "\n",
        "Note: load_boston() is deprecated. Use an alternative like fetch_california_housing() if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pj7n-yBTWWp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Python Program (GridSearchCV Tuning on Iris Dataset)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ],
      "metadata": {
        "id": "AYOs87gqWeC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "4cTvKvhmWlsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Healthcare Use Case – Step-by-Step Process\n",
        "\n",
        "Answer:\n",
        "\n",
        "Step 1: Handle Missing Values\n",
        "\n",
        "Use imputation strategies (mean/median for numerical, mode for categorical)\n",
        "\n",
        "Optionally drop rows/columns with too many missing values\n",
        "\n",
        "\n",
        "Step 2: Encode Categorical Features\n",
        "\n",
        "Use Label Encoding or One-Hot Encoding depending on model needs\n",
        "\n",
        "\n",
        "Step 3: Train a Decision Tree Model\n",
        "\n",
        "Split data into train/test\n",
        "\n",
        "Use DecisionTreeClassifier() and fit on training data\n",
        "\n",
        "\n",
        "Step 4: Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV to find the best max_depth, min_samples_split, etc.\n",
        "\n",
        "\n",
        "Step 5: Evaluate Performance\n",
        "\n",
        "Use metrics like Accuracy, Precision, Recall, F1-score, Confusion Matrix\n",
        "\n",
        "\n",
        "Business Value:\n",
        "\n",
        "Enables early and accurate disease prediction\n",
        "\n",
        "Supports better patient care and treatment planning\n",
        "\n",
        "Saves costs and improves efficiency in medical diagnosis\n"
      ],
      "metadata": {
        "id": "LOnSW92VWs61"
      }
    }
  ]
}